# 标题待定

工作用技术栈记录

参考技术栈为开源项目https://gitee.com/xiaoxiangopen/analysis



# Flume数据采集

## 前言

公司平台每日会产生大量日志，一般为流式数据，如查询、搜索引擎pv等，处理这些日志需要特定日志系统，需要具备以下特征：

1. 应用系统与分析系统的桥梁，解耦关联
2. 近实时的在线分析系统，类似Hadoop的离线分析系统
3. 高可扩展性，如数据量增加时，可以通过增加节点进行水平扩展

**符合条件的开源日志系统**

- scribe（facebook）
- chukwa（apache）
- flume（cloudera）

## 简介

分布式、可靠、高可用的海量日志聚合系统。（日志收集框架）

- 支持在系统中定制各类数据发送方，用于收集数据；
- 提供对数据的简单处理，写到各种数据接收方（比如文本、HDFS、Hbase等）的能力。

**可靠性**

节点故障时，能将日志传送到其他节点不会丢失；

提供三种级别可靠性：

- end-to-end 

  收到数据agent首先将event写到磁盘上，当数据传送成功后，再删除；如果数据发送失败，可以重新发送。

- Store on failure 

  也是scribe采用的策略，当数据接收方crash时，将数据写到本地，待恢复后，继续发送

- Besteffort 

  数据发送到接收方后，不会进行确认

**可恢复性**

靠Channel，推荐使用FileChannel，事件持久化在本地文件系统里（性能较差）



## 安装

- 前提：JDK环境
- 下载：http://archive.apache.org/dist/flume/ 当前为[apache-flume-1.10.1-bin.tar.gz](http://www.apache.org/dyn/closer.lua/flume/1.10.1/apache-flume-1.10.1-bin.tar.gz)

#### Win

目标：创建一个简单例子监听44444端口的输入并在console中输出

- 解压缩，为了方便后续，配置环境变量

  ```shell
  FLUME_HOME=#flume根目录
  # path添加 %FLUME_HOME%\conf;%FLUME_HOME%\bin;
  ```

  ![image-20220830150937322](https://strangest.oss-cn-shanghai.aliyuncs.com/markdown/202208301509346.png)

  可以通过控制台输入指令测试

  ```shell
  flume-ng version
  ```

  ![image-20220830151018158](https://strangest.oss-cn-shanghai.aliyuncs.com/markdown/202208301510183.png)

- 在conf目录下创建test.conf

  ```properties
  # 参考flume-conf.properties.template
  # 定义这个 agent 中各组件的名字 r
  a1.sources = r1 
  a1.sinks = k1 
  a1.channels = c1 
  # 描述和配置 source 组件：r1 
  a1.sources.r1.type = netcat 
  a1.sources.r1.bind = localhost 
  a1.sources.r1.port = 44444 
  # 描述和配置 sink 组件：k1 
  a1.sinks.k1.type = logger 
  # 描述和配置channel组件，此处使用是内存缓存的方式 
  a1.channels.c1.type = memory 
  a1.channels.c1.capacity = 1000 
  a1.channels.c1.transactionCapacity = 100 
  # 描述和配置 source channel sink 之间的连接关系 
  a1.sources.r1.channels = c1 
  a1.sinks.k1.channel = c1 
  ```

- 控制台进入bin目录，运行指令启动

  ```shell
  # 任选其一
  flume-ng agent --conf ../conf --conf-file ../conf/test.conf --name a1 -property flume.root.logger=INFO,console
  flume-ng agent --conf %FLUME_HOME%/conf --conf-file %FLUME_HOME%/conf/test.conf --name a1 -property flume.root.logger=INFO,console
  # powershell 运行  利用环境变量
  flume-ng.cmd agent --conf $env:FLUME_HOME/conf --conf-file $env:FLUME_HOME/conf/test.conf --name a1 -property "flume.root.logger=INFO,console"
  flume-ng agent -conf $env:FLUME_HOME/conf -conf-file $env:FLUME_HOME/conf/test.conf -name a1 -property "flume.root.logger=INFO,console"
  ```

  ![image-20220830153507174](https://strangest.oss-cn-shanghai.aliyuncs.com/markdown/202208301535200.png)

  虽然有报错，但从端口查看是已经启动了

- 启动另一个控制台，使用telnet连接到44444端口并发送信息Hello World!

  ```shell
  telnet localhost 44444
  ```

  ![image-20220830153920744](https://strangest.oss-cn-shanghai.aliyuncs.com/markdown/202208301539764.png)

  查看flume日志（flume.log，在哪个目录运行指令就在哪个目录）

  ![image-20220830153951263](https://strangest.oss-cn-shanghai.aliyuncs.com/markdown/202208301539290.png)

> telnet需要开启后才能用
>
> https://help.aliyun.com/document_detail/40796.html



##### 1.10.1版本当前配置不显示控制台日志

1.9.0可以显示，从显示内容来看因为配置未生效

原因是1.9之前使用log4j2.properties文件配置，1.10使用log4j2.xml配置，用于对应log4j 2.X版本。

```xml
<?xml version="1.0" encoding="UTF-8"?>
<!--...-->
<Configuration status="ERROR">
  <Properties>
    <Property name="LOG_DIR">.</Property>
  </Properties>
  <Appenders>
    <Console name="Console" target="SYSTEM_ERR">
      <PatternLayout pattern="%d (%t) [%p - %l] %m%n" />
    </Console>
    <RollingFile name="LogFile" fileName="${LOG_DIR}/flume.log" filePattern="${LOG_DIR}/archive/flume.log.%d{yyyyMMdd}-%i">
      <PatternLayout pattern="%d{dd MMM yyyy HH:mm:ss,SSS} %-5p [%t] (%C.%M:%L) %equals{%x}{[]}{} - %m%n" />
      <Policies>
        <!-- Roll every night at midnight or when the file reaches 100MB -->
        <SizeBasedTriggeringPolicy size="100 MB"/>
        <CronTriggeringPolicy schedule="0 0 0 * * ?"/>
      </Policies>
      <DefaultRolloverStrategy min="1" max="20">
        <Delete basePath="${LOG_DIR}/archive">
          <!-- Nested conditions: the inner condition is only evaluated on files for which the outer conditions are true. -->
          <IfFileName glob="flume.log.*">
            <!-- Only allow 1 GB of files to accumulate -->
            <IfAccumulatedFileSize exceeds="1 GB"/>
          </IfFileName>
        </Delete>
      </DefaultRolloverStrategy>
    </RollingFile>
  </Appenders>

  <Loggers>
    <Logger name="org.apache.flume.lifecycle" level="info"/>
    <Logger name="org.jboss" level="WARN"/>
    <Logger name="org.apache.avro.ipc.netty.NettyTransceiver" level="WARN"/>
    <Logger name="org.apache.hadoop" level="INFO"/>
    <Logger name="org.apache.hadoop.hive" level="ERROR"/>
    <Root level="INFO">
      <AppenderRef ref="LogFile" />
    </Root>
  </Loggers>
</Configuration>
```

根据配置，可以看到logger里引入的appender才能生效，而当前虽然有console的配置，但并没有加入生效行列。

所以修改loggers配置为

```xml
<Loggers>
    <Logger name="org.apache.flume.lifecycle" level="info"/>
    <Logger name="org.jboss" level="WARN"/>
    <Logger name="org.apache.avro.ipc.netty.NettyTransceiver" level="WARN"/>
    <Logger name="org.apache.hadoop" level="INFO"/>
    <Logger name="org.apache.hadoop.hive" level="ERROR"/>
    <Root level="INFO">
        <AppenderRef ref="LogFile" />
        <AppenderRef ref="Console" />
    </Root>
</Loggers>
```

然后简化指令启动即可

```shell
flume-ng agent  -conf $env:FLUME_HOME/conf -conf-file $env:FLUME_HOME/conf/test.conf -name a1
```



### Linux

- 解压缩到自定义位置

- 配置环境变量，插入/etc/profile如下内容（或者/etc/profile.d/目录下新建**.sh）

  ```shell
  export FLUME_HOME=/xxx/apache-flume-1.9.0-bin
  
  export FLUME_CONF_DIR=$FLUME_HOME/conf
  
  export PATH=$FLUME_HOME/bin:$PATH
  ```

  别忘了刷新环境变量`source /etc/profile`

- conf目录拷贝一个flume-env.sh.template为flume-env.sh，将jdk位置配置进去

  ```sh
  export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
  ```

  > 如果你环境变量里把jdk位置用JAVA_HOME配置过了可以省略这一步

- conf目录拷贝一个flume-conf.properties.template为flume-conf.properties，配置文件，

  这里以采集nginx访问日志并将其打印到控制台为例

  ```properties
  # 定义一个名为a1的agent中各组件的名字 
  a1.sources = r1 
  a1.sinks = k1 
  a1.channels = c1 
   
  # 描述和配置 source 组件：r1 
  a1.sources.r1.type = exec
  a1.sources.r1.command = tail -F /var/log/nginx/access.log
   
  # 描述和配置 sink 组件：k1 
  a1.sinks.k1.type = logger 
   
  # 描述和配置 channel 组件，此处使用是内存缓存的方式 
  a1.channels.c1.type = memory 
  a1.channels.c1.capacity = 1000 
  a1.channels.c1.transactionCapacity = 100 
   
  # 描述和配置 source、channel、sink 之间的连接关系 
  a1.sources.r1.channels = c1 
  a1.sinks.k1.channel = c1
  ```

- 在flume根目录启动flume（这个影响后面的相对路径，按需修改）

  ```bash
  ./bin/flume-ng agent -n a1 -c conf -f conf/flume-conf.properties
  
  -n a1 指定 agent 的名字
  -c conf 指定配置文件目录
  -f conf/nginx-logger.conf 指定配置文件
  ```

  > 注意默认会生成一个flume.log日志文件，根据你执行指令的位置放的



## 运行机制

Flume 的核心是把数据从数据源(source)收集过来，在将收集到的数据送到指定的目的地(sink)。

为了保证输送的过程一定成功，在送到目的地(sink)之前，会先缓存数据(channel)，待数据真正到达目的地(sink)后，flume 再删除自己缓存的数据。

Flume 分布式系统中核心的角色是 agent，agent 本身是一个 Java 进程，一般运行在日志收集节点。

flume 采集系统就是由一个个 agent 所连接起来形成。

![img](https://strangest.oss-cn-shanghai.aliyuncs.com/markdown/202209080919599.webp)

### Agent

每一个 agent 相当于一个数据传递员，内部有三个组件：

- Source：采集源，用于跟数据源对接，以获取数据；
- Sink：下沉地，采集数据的传送目的，用于往下一级 agent 传递数据或者往最终存储系统传递数据；
- Channel：agent 内部的数据传输通道，用于从 source 将数据传递到 sink；

### Event

在整个数据的传输的过程中，流动的是 event，它是 Flume 内部数据传输的最基本单元。

event 将传输的数据进行封装，如果是文本文件，通常是一行记录，event 也是事务的基本单位。

event 从 source，流向 channel，再到 sink，本身为一个字节数组，并可携带headers(头信息)信息。

event 代表着一个数据的最小完整单元，从外部数据源来，向外部的目的地去。 

一个完整的 event 包括：event  headers、event  body、event 信息，其中event 信息就是 flume 收集到的日记记录。





# Hadoop



# Flink

## 简介

Apache软件基金会开发的开源流处理框架，其核心是用Java和Scala编写的分布式流数据流引擎。

Flink以数据并行和流水线方式执行任意流数据程序，Flink的流水线运行时系统可以执行批处理和流处理程序。

> - 批处理：主要操作**大容量静态数据集**，并在计算过程完成后**返回结果**。 用于处理离线数据，冷数据。单个处理数据量大，处理速度比流慢。
> - 流处理：对**随时进入系统**的数据进行计算。在线，实时产生的数据。单次处理的数据量小，但处理速度更快。

此外，Flink的运行时本身也支持迭代算法的执行。

Flink程序在执行后被映射到流数据流，每个Flink数据流以一个或多个源（数据输入，例如消息队列或文件系统）开始，并以一个或多个接收器（数据输出，如消息队列、文件系统或数据库等）结束。

Flink可以对流执行任意数量的变换，这些流可以被编排为有向无环数据流图，允许应用程序分支和合并数据流。

## 安装

windows版本目前没有直接脚本运行，可以考虑win运行linux程序再执行。

### Linux

- 下载 https://archive.apache.org/dist/flink/ 

  目前选择版本 1.15.2 flink-1.15.2-bin-scala_2.12.tgz    并解压缩

- 进入bin目录，运行脚本，启动本地集群

  ```bash
  $ ./bin/start-cluster.sh
  ```

- 关闭本地集群

  ```bash
  $ ./bin/stop-cluster.sh
  ```

  





# Clickhouse

## 简介

俄罗斯的 Yandex 于2016年开源的列式存储数据库（DBMS)，主要用于在线分析处理查询（OLAP），能够使用SQL 查询实时生成分析数据报告。





# kafka

## 简介

是一个分布式、分区的、多副本的、多订阅者，基于zookeeper协调的分布式日志系统（也可以当做MQ系统），常见可以用于web/nginx日志、访问日志，消息服务等。

主要应用场景：日志收集系统和消息系统



## 安装

### Win

- 安装Zookeeper，下载地址https://mirrors.bfsu.edu.cn/apache/zookeeper/  

  这里选择版本 3.8.0对应的apache-zookeeper-3.8.0-bin.tar.gz 并解压缩

  > 从3.5.5开始，带有bin名称的包才能解压出直接运行的包

  - 进入conf目录，复制一份zoo_sample.cfg  重命名为zoo.cfg
  - 进入bin目录，运行zkServer.cmd即可

  > 现在的kafka存在内置zookeeper版本的启动，所以这一步可以省略

- 下载kafka，下载地址https://mirrors.bfsu.edu.cn/apache/kafka/ 

  这里选择版本3.2.1对应的kafka_2.13-3.2.1.tgz 并解压缩

- 进入bin/windows目录，启动zookeeper，进行这一步则不用走第一步

  ```powershell
  .\zookeeper-server-start.bat ..\..\config\zookeeper.properties
  ```

- 还在bin/windows目录，启动kafka

  ```powershell
  .\kafka-server-start.bat ..\..\config\server.properties
  ```


### Linux

- 下载，解压缩

- 启动zookeeper，用自带的

  ```bash
  > bin/zookeeper-server-start.sh config/zookeeper.properties
  ```

- 启动kafka

  ```bash
  > bin/kafka-server-start.sh config/server.properties
  ```



## 初步使用

### 创建一个topic

创建一个名为“quickstart-events”的topic

```bash
> bin/kafka-topics.sh --create --topic quickstart-events --bootstrap-server localhost:9092
```

运行list（列表）命令来查看这个topic：

```bash
> bin/kafka-topics.sh --describe --topic quickstart-events --bootstrap-server localhost:9092
```

![image-20220908150856612](https://strangest.oss-cn-shanghai.aliyuncs.com/markdown/202209081508815.png)



### 把event写入topic

通过通信写入事件，永久存储。

控制台指令将event写入topic，默认情况一行一个事件

```bash
> bin/kafka-console-producer.sh --topic quickstart-events --bootstrap-server localhost:9092
# 进入写事件状态
This is my first event
This is my second event
# Ctrl+C退出
```



### 读取event

```bash
bin/kafka-console-consumer.sh --topic quickstart-events --from-beginning --bootstrap-server localhost:9092
```

![image-20220908151814313](https://strangest.oss-cn-shanghai.aliyuncs.com/markdown/202209081518355.png)



### 终止

- 通过`Ctrl+C`关闭生产者与消费者
- 关闭Kafka
- 关闭ZooKeeper

> 如果删除存储的数据，存储的event
>
> ```
> rm -rf /tmp/kafka-logs /tmp/zookeeper
> ```
